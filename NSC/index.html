
<!DOCTYPE html>
<html>
<head>
<style>   
    figure {
    display: inline-block;    
    }

figure img {
    float:left;
    margin: 0;
}
figure figcaption {   
    text-align: center;
}
caption { 
    display: table-caption;
    text-align: center!important;
}

.figure > figure {
    text-align: center;
    float:  left;
    width: 33%;
}

.figure > figure img {
    width: 99%;
}

.crop {

    width: 400px;
    height: 300px;
    overflow: hidden;
}

.crop img {
    text-align: center;
    height: 300px;
    margin: -25px 0 0 0px;
}

</style>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta property="og:url"                content="https://mashyu.github.io/NSC/" />
    <meta property="og:type"               content="article" />
    <meta property="og:title"              content="Netizen-Style Commenting on Fashion Photos – Dataset and Diversity Measures" />
    <meta property="og:description"        content="Wen Hua Lin, Kuan-Ting Chen, Hung Yueh Chiang, Winston Hsu" />
    <meta property="og:image" content="https://mashyu.github.io/NSC/Samples/header_img.jpg" />
    <meta property="og:image:secure_url" content="https://mashyu.github.io/NSC/Samples/header_img.jpg" />
    <meta property="og:image:width"        content="640" /> 
    <meta property="og:image:height"       content="442" />
    
    <title> </title>
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-52062463-2', 'auto');
        ga('send', 'pageview');
    </script>
    <script>
        $(document).ready(function(){
            $('.download').on('click', function() {
                ga('send', 'event', 'Downloads', 'Download', $(this).attr('id'));
            });
        });
    </script>
</head>

<body>
<div class="container">
    <div class="row">
        <div class="col-md-12">
            <h2>Netizen-Style Commenting on Fashion Photos – Dataset and Diversity Measures</h2>
            <h4 align="center">
            <!--a href="https://lafi.github.io">Meng-Ru Hsieh</a--> 
            <a href="mailto:q868686qq@gmail.com">Wen Hua Lin</a>, 
            <a href="https://kutichen.github.io/ktchen/">Kuan-Ting Chen</a>,
            <a href="mailto:kenny5312012@gmail.com">Hung Yueh Chiang</a>,
            <a href="http://winstonhsu.info/">Winston Hsu</a>
            </h4>
        </div>
    </div>

<div style=" padding-left: 100px">
<figure style="padding-top: 30px;">
    <img style="margin-bottom: 0px;" src='./Samples/header_img.jpg' alt='missing' width="1000" />
</figure>
    </div>
<div style="clear:both;"></div>


    <div class="row">
        <div class="col-md-12">
            <h3>Abstract</h3>
            Recently, image captioning has appeared promising, which is expected to widely apply in chatbot area. Yet, “vanilla” sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagement with users. Hence, we propose Netizen Style Commenting (NSC), to generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid netizen style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. 
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <h3>Publication</h3>
            <ul>
            <li>
            <p>
            Wen Hua Lin, Kuan-Ting Chen, HungYueh Chiang, Winston H. Hsu. Netizen-Style Commenting on Fashion Photos – Dataset and Diversity Measures, WWW 2018 
            [<a href="https://arxiv.org/abs/1801.10300">arXiv pdf</a>]
            <!--[<a data-toggle="collapse" data-target="#bibtex">bibtex</a>]-->
            <div id="bibtex" class="well collapse">
                @inproceedings{Hsieh_2017_ICCV, <br />
                Author = {Meng-Ru Hsieh and Yen-Liang Lin and Winston H. Hsu}, <br />
                Booktitle = {The IEEE International Conference on Computer Vision (ICCV)}, <br />
                Title = {Drone-based Object Counting by Spatially Regularized Regional Proposal Networks}, <br />
                Year = {2017}, <br />
                organization={IEEE} <br />
                } 
            </div>
            </p>
            </li>
            </ul>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <h3>Dataset</h3>
            <h5 class="text-danger"> Please notice that this dataset is made available for academic research purpose only. Images are collected from <a href="http://lookbook.nu/">Lookbook</a>, and the copyright belongs to the original owners. If any of the images belongs to you and you would like it removed, please kindly inform <a href="mailto:q868686qq@gmail.com">us</a>, we will remove it from our dataset immediately.</h5>
            <!--<img src="./images/dataset_comparison.jpg" class="img-responsive" />-->
            <h4>NetiLook Dataset</h4>
            NetiLook  contains 355,205 images from 11,034 users and 5 million associated comments collected from Lookbook. 
            The downloaded dataset contain following structures:
            <div class="well">
                <ul class="list-unstyled">
                    <li>Images - contains 355,205 images</li>
                    <li>Examples:</li>
                        <ul>
                        <li>123456789 is post ID</li>
                        <li>You can file image of it in img/123/456/789/01.jpg</li>
                        <!--li>videos      - The drone view videos (*.mov) filmed over the parking lots</li-->
                        </ul>
                    <ul><br /></ul>
                    <li>Metadata - information about posts in SQL format
                    <li>There two table in the SQL file</li> 
                    </li>
                        <ul>
                        <li>Posts</li>
                        <ul><li>Information about posts</li></ul>
                        <li>Users</li>
                        <ul><li>Information about the owners of posts</li></ul>
                        </ul>
                    <ul><br /></ul>
                    <li>Content of Phanton, Items and Comments in Posts are JSON format.</li>
                    <li>Examples:</li>
                        <ul>
                        <li>Phanton</li>
                            <ul><li>[{"#ead0cd": "offwhite"}, {"#000000": "black"}, {"#8c4600": "brown"}]</li></ul>
                        <li>Items</li>
                            <ul><li>[[{"Name": "Shirt"}, {"Brand": null}, {"Store": null}, {"X": 221}, {"Y": 203}]]</li></ul>
                            <ul><li>X and Y are the position of the item in the image taged by the user</li></ul>
                        <li>Comments</li>
                            <ul><li>[[{"Name": "Tonka K."}, {"Url": "http://lookbook.nu/tonkakatie"}, {"Comment": "nice wegdes :D hype! check out my new look! ;))"}]]</li></ul>
                        </ul>
                </ul>
            </div>
            <ul>
            <li>All files are encoded with a password. Please, read and fill in the online <a href="https://docs.google.com/forms/d/e/1FAIpQLScjPoEUMgiLP7DqhoIzWR-zp_JYqPOuze-jkm8I7OaDy3T6QQ/viewform?usp=sf_link" class="download" id="datasets_and_materials">EULA form</a> before downloading the database. Once we've received the EULA form, we will e-mail you the password for the files.</li>

            <li>Download images of NetiLook dataset from [<a href="https://drive.google.com/open?id=1AIePKwofYkfSUpCSvMMEUrP6l7CSy1l8" class="download" id="datasets_and_materials">here</a>]</li>
            <li>Download metadata of NetiLook dataset from [<a href="https://drive.google.com/open?id=10vpJJKVj-yNHIJ9QVS8w65TJMot9hSvA" class="download" id="datasets_and_materials">here</a>]</li>
            <li>Download file list and ground truth used in our experiment from [<a href="https://drive.google.com/open?id=1jnpFwa3zors8Ugbqo2-LsSG3OBZHuN15" class="download" id="datasets_and_materials">here</a>]</li>
            <li>If you have any problems downloading the data, do not hesitate to contact us at <a href="mailto:q868686qq@gmail.com">my e-mail address</a>.</li>
            <!--li>The datasets and all related materials will be made publically available soon.</li-->
            
            </ul>
            <p>
            <h5>Notes</h5>
            *For evaluation of human, we pick one sentence from the dataset as the answer of human and others as the ground truth.<br>
            *For comparision with scores of human, we pick four sentences from ground truth to evaluate methods.<br>
            </p>    
            <br/>


            <h4>Dataset description</h4>
            <p>
            To the best of our knowledge, this is the first and the largest netizen-style commenting dataset. It contains 355,205 images from 11,034 users and 5 million associated comments collected from Lookbook. As the examples shown in Figure 1, most of the images are fashion photos in various angles of views, distinct filters and different styles of collage. As Figure 2 (b) shows, each image is paired with (diverse) user comments, and the average number of comments is 14 per image in our dataset. Besides, each post has a title named by an author, a publishing date and the number of hearts given by other users. Moreover, some users add names, brands, pantone of the clothes, and stores where they bought the clothes. Furthermore, we collect the authors’ public information. Some of them contain age, gender, country and the number of fans (cf., Figure 3). In this paper, we only use the comments and the photos from our dataset. Other attributes can be used to refine the system in future work. For comparing the results on Flickr30k, we also sampled 28,000 for training, 1,000 for validation and 1,000 for testing. Besides, we also sampled five comments for each image.
            </p>
            <p>
            Compared to general image captioning datasets such as Flickr30k (Rashtchian et al. 2010), the data from social media are quite noising, full of emojis, emoticons, slang and much shorter (cf., Figure 2 (b) and Table 1), which makes generating a vivid netizen style comment much more challenging. Moreover, plenty of photos are in different styles of collage (cf., photos in Figure 1). Therefore, it makes the image features much more noising than single view photos.
            </p>


<div class="figure">
<figure style="padding-top: 30px;">
    <img style="margin-bottom: 21px;" src='./images/dataset/Fig1.jpg' alt='missing'/>
    <figcaption>Figure 1</figcaption>
</figure>

<figure style="padding-top: 30px;">
    <img style="margin-bottom: 43px;" src='./images/dataset/Fig2.jpg' alt='missing'/>
    <figcaption>Figure 2</figcaption>
</figure>

<figure>
    <img src='./images/dataset/Fig3.jpg' alt='missing'/>
    <figcaption>Figure 3</figcaption>
</figure>
<div style="clear:both;"></div>
</div>
            
            

            <div class="table-responsive">
            <table class="table table-bordered" style="margin-top: 40px;">
            <caption text-align="center" align="bottom" >Table 1</caption>
                <tr>
                    <td align="center">Dataset</td>
                    <td align="center">Images</td>
                    <td align="center">Sentences</td>
                    <td align="center">Average Length</td>
                    <td align="center">Unique words</td>
                </tr>
                <tr>
                    <td align="center">Flickr30k</td>
                    <td align="center">30K</td>
                    <td align="center">150K</td>
                    <td align="center">13.39</td>
                    <td align="center">23,461</td>
                </tr>
                <tr>
                    <td align="center">MS COCO</td>
                    <td align="center">200K</td>
                    <td align="center">1M</td>
                    <td align="center">10.46</td>
                    <td align="center">54.231</td>
                </tr>
                <tr>
                    <td align="center">NetiLook</td>
                    <td align="center">350K</td>
                    <td align="center">5M</td>
                    <td align="center">3.75</td>
                    <td align="center">597,629</td>
                </tr>
            </table>
            </div>
        </div>
    </div>
    <div class="row">
        <div class="col-md-12">
            <h3>Experimental Results</h3>
            <h4>Performance on Flickr30k testing splits</h4>
            In Flickr30k, style-weight does not improve much in diversity, because the sentences are objectively depicting humans performing various activities.
            <div class="table-responsive">
            <table class="table table-bordered">
                <tr>
                    <td>Method</td>
                    <td>BLUE-4</td>
                    <td>METEOR</td>
                    <td>WF-KL</td>
                    <td>POS-KL</td>
                    <td>DicRate</td>
                </tr>
                <tr>
                    <td>Human</td>
                    <td>0.108</td>
                    <td>0.235</td>
                    <td>1.090</td>
                    <td>0.013</td>
                    <td>0.664</td>
                </tr>
                <tr>
                    <td>NC</td>
                    <td>0.094</td>
                    <td>0.147</td>
                    <td>1.215</td>
                    <td>0.083</td>
                    <td>0.216</td>
                </tr>
                <tr>
                    <td>Attention</td>
                    <td>0.121</td>
                    <td>0.148</td>
                    <td>1.203</td>
                    <td>0.302</td>
                    <td>0.053</td>
                </tr>
                <tr>
                    <td>NSC<sub>NC</sub></td>
                    <td>0.089</td>
                    <td>0.146</td>
                    <td>1.217</td>
                    <td>0.075</td>
                    <td>0.228</td>
                </tr>
                <tr>
                    <td>NSC<sub>Attention</sub></td>
                    <td>0.119</td>
                    <td>0.148</td>
                    <td>1.202</td>
                    <td>0.319</td>
                    <td>0.055</td>
                </tr>
            </table>
            </div>
            <h4>Performance on NetiLook testing splits</h4>
            Style-weight can guide the generating process to the comment that is much closer to the users’ behaviour in the social media, making machine mimic online netizen comment style and culture.
            <div class="table-responsive">
            <table class="table table-bordered">
                <tr>
                    <td>Method</td>
                    <td>BLUE-4</td>
                    <td>METEOR</td>
                    <td>WF-KL</td>
                    <td>POS-KL</td>
                    <td>DicRate</td>
                </tr>
                <tr>
                    <td>Human</td>
                    <td>0.008</td>
                    <td>0.172</td>
                    <td>0.551</td>
                    <td>0.004</td>
                    <td>0.381</td>
                </tr>
                <tr>
                    <td>NC</td>
                    <td>0.013</td>
                    <td>0.151</td>
                    <td>0.665</td>
                    <td>1.126</td>
                    <td>0.036</td>
                </tr>
                <tr>
                    <td>Attention</td>
                    <td>0.020</td>
                    <td>0.133</td>
                    <td>0.639</td>
                    <td>1.629</td>
                    <td>0.011</td>
                </tr>
                <tr>
                    <td>NSC<sub>NC</sub></td>
                    <td>0.013</td>
                    <td>0.172</td>
                    <td>0.695</td>
                    <td>0.376</td>
                    <td>0.072</td>
                </tr>
                <tr>
                    <td>NSC<sub>Attention</sub></td>
                    <td>0.030</td>
                    <td>0.139</td>
                    <td>0.659</td>
                    <td>1.892</td>
                    <td>0.012</td>
                </tr>
            </table>
            </div>
            <h4>
                User Study
            </h4>
            <p>
                To demonstrate the effect of diverse comments, we conducted a user study from 23 users. The users are about 25 year-old and familiar with netizen style community and social media. The sex ratio in our user study is 2.83 males/female. They are asked to rank comments for 35 fashion photos. Each photo has 4 comments — from one randomly picked human comments, NC, Attention and our NSCNC. Therefore, each of the users has to appraise 140 comments generated from different methods. Furthermore, we collect user feedback to understand user judgements on comments generated by different methods. According to the result, our style-weight makes captioning model mimic human style and generates human-like comments which most people agree with in our user study.
            </p>
            <div class="table-responsive">
            <table class="table table-bordered">
                <tr>
                    <td>Ranking</td>
                    <td>Human</td>
                    <td>NC</td>
                    <td>Attention</td>
                    <td>NSC<sub>NC</sub></td>
                </tr>
                <tr>
                    <td>Rank 1</td>
                    <td>46.1%</td>
                    <td>10.8%</td>
                    <td>6.3%</td>
                    <td>36.8%</td>
                </tr>
                <tr>
                    <td>Rank 2</td>
                    <td>24.5%</td>
                    <td>21.4%</td>
                    <td>14.4%</td>
                    <td>39.8%</td>
                </tr>
                <tr>
                    <td>Rank 3</td>
                    <td>18.1%</td>
                    <td>31.9%</td>
                    <td>34.3%</td>
                    <td>15.7%</td>
                </tr>
                <tr>
                    <td>Rank 4</td>
                    <td>11.3%</td>
                    <td>35.9%</td>
                    <td>45.0%</td>
                    <td>7.8%</td>
                </tr>
            </table>
            </div>
        </div>
    </div>
    
    <div class="row">
        <div class="col-md-12">
            <h5>Reference</h5>
            1. Lin, Tsung-Yi, et al. "Microsoft coco: Common objects in context." European conference on computer vision. Springer, Cham, 2014.<br/>
            2. Rashtchian, Cyrus, et al. "Collecting image annotations using Amazon's Mechanical Turk." Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk. Association for Computational Linguistics, 2010.<br/>
            3. Vinyals, Oriol, et al. "Show and tell: A neural image caption generator." Proceedings of the IEEE conference on computer vision and pattern recognition. 2015.<br/>
            4. Xu, Kelvin, et al. "Show, attend and tell: Neural image caption generation with visual attention." International Conference on Machine Learning. 2015.<br/>
            5. The photos used in this webiste was collected from lookbook.nu for community research and the copyright belongs to the original owners.
            <br />
        </div>
    </div>

    <div class="row">
        <div class="col-md-12">
            <h3>Acknowledgement</h3>
            <p>
            This work was supported in part by Microsoft Research Asia and the Ministry of Science and Technology, Taiwan, under Grant MOST 105-2218-E-002-032. We also benefit from the grants from NVIDIA and the NVIDIA DGX-1 AI Supercomputer and the discussions with Dr. Ruihua Song, Microsoft Research Asia.
            </p>
        </div>
    </div>


    <hr>
</div>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
</body>
</html>
